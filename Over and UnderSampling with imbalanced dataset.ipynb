{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of balancing an imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing classification algorithms, the distrubtion of your dataset is important. For every obseravable output of your predictions performance realise on the number of observations for each potiental output.\n",
    "\n",
    "\n",
    "For example, lets say the majority (most obseravations)  class is clearly dominating the other classes and the decision boundaries for the other classes are barely recognizable. As we keep adding more observations to the underrepresented classes, the decision boundaries change dramatically. Thus, when trying to make predictions with regards to a minority class, we need to find ways to avoid being misled by the majority class.\n",
    "\n",
    "\n",
    "        Let's define a simple way to keep track of the result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that dataset we will be using the wine quality dataset which is provided by imblearn package under their dataset, let's explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_datasets()['wine_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 7.  ,  0.27,  0.36, ...,  3.  ,  0.45,  8.8 ],\n",
       "        [ 6.3 ,  0.3 ,  0.34, ...,  3.3 ,  0.49,  9.5 ],\n",
       "        [ 8.1 ,  0.28,  0.4 , ...,  3.26,  0.44, 10.1 ],\n",
       "        ...,\n",
       "        [ 6.5 ,  0.24,  0.19, ...,  2.99,  0.46,  9.4 ],\n",
       "        [ 5.5 ,  0.29,  0.3 , ...,  3.34,  0.38, 12.8 ],\n",
       "        [ 6.  ,  0.21,  0.38, ...,  3.26,  0.32, 11.8 ]]),\n",
       " 'target': array([-1, -1, -1, ..., -1, -1, -1], dtype=int64),\n",
       " 'DESCR': 'wine_quality'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(data.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -1 4715]\n",
      " [   1  183]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through simple analysis we can conclude that the majority class (-1) has 4715 counts as opposed to the minority with 183. This means that the dataset is imbalanced and that any decision boundary made on this may mislead predicition performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Classification-  We will now be using the random forest classification with additional parameters for under and over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE and NEARMISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.\n",
    "\n",
    "\n",
    "whereas NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darman\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Darman\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Darman\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# build normal model\n",
    "pipeline = make_pipeline(classifier(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# build model with SMOTE imblearn\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), classifier(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "# build model with undersampling\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the model we'd like to check the validation metric for the model, this means evualating the model with metrics that display the distrubtion of the data pre and post processing as well as the classificaation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "normal data distribution: Counter({-1: 4715, 1: 183})\n",
      "SMOTE data distribution: Counter({-1: 4715, 1: 4715})\n",
      "NearMiss data distribution: Counter({-1: 183, 1: 183})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.99      0.98      1182\n",
      "           1       0.46      0.14      0.21        43\n",
      "\n",
      "    accuracy                           0.96      1225\n",
      "   macro avg       0.72      0.57      0.60      1225\n",
      "weighted avg       0.95      0.96      0.95      1225\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.98      0.97      0.33      0.97      0.56      0.34      1182\n",
      "          1       0.31      0.33      0.97      0.32      0.56      0.30        43\n",
      "\n",
      "avg / total       0.95      0.95      0.35      0.95      0.56      0.34      1225\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.96      0.35      0.65      0.51      0.47      0.22      1182\n",
      "          1       0.03      0.65      0.35      0.07      0.47      0.23        43\n",
      "\n",
      "avg / total       0.93      0.36      0.64      0.49      0.47      0.22      1225\n",
      "\n",
      "\n",
      "normal Pipeline Score 0.9640816326530612\n",
      "SMOTE Pipeline Score 0.9510204081632653\n",
      "NearMiss Pipeline Score 0.35591836734693877\n",
      "\n",
      "normal classification\n",
      "accuracy: 0.9640816326530612\n",
      "precision: 0.46153846153846156\n",
      "recall: 0.13953488372093023\n",
      "f1: 0.21428571428571427\n",
      "\n",
      "SMOTE classification\n",
      "accuracy: 0.9510204081632653\n",
      "precision: 0.3111111111111111\n",
      "recall: 0.32558139534883723\n",
      "f1: 0.3181818181818182\n",
      "\n",
      "NearMiss classification\n",
      "accuracy: 0.35591836734693877\n",
      "precision: 0.034912718204488775\n",
      "recall: 0.6511627906976745\n",
      "f1: 0.06627218934911241\n"
     ]
    }
   ],
   "source": [
    "# print information about both models\n",
    "print()\n",
    "print(\"normal data distribution: {}\".format(Counter(data['target'])))\n",
    "X_smote, y_smote = SMOTE().fit_sample(data['data'], data['target'])\n",
    "print(\"SMOTE data distribution: {}\".format(Counter(y_smote)))\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(data['data'], data['target'])\n",
    "print(\"NearMiss data distribution: {}\".format(Counter(y_nearmiss)))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, prediction))\n",
    "print(classification_report_imbalanced(y_test, smote_prediction))\n",
    "print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "\n",
    "print()\n",
    "print('normal Pipeline Score {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('SMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))\n",
    "print('NearMiss Pipeline Score {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print()\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal classifier without any over and undersampling has a high accuracy meaning the model is accurate but in this case if we were predicting the outcome of the  minority class, the best metric would be a tradeoff between precision and recall. \n",
    "\n",
    "Recall which means how well our model can predict all the interested samples in our dataset. whereas precision is  a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
